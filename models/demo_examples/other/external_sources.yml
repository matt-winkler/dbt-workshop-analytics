#
#  Do you have data in S3? Do you want to access this data within Snowflake from dbt?
#  Well you can, with the dbt-external-tables package!
#
#  GUIDE
#
#  (1) Create Snowflake external stages, e.g.,:
#
#       use database analytics;
#       use schema <your dev or prod schema>;
#       create or replace stage s3_dbt_tutorial_public url='s3://dbt-tutorial-public/' file_format = (type=csv);
# 
#  (2) Add the dbt-external-tables package in packages.yml
# 
#  (3) Execute `dbt run-operation stage_external_sources` to create the external tables,
#      and to refresh them later. (Add this to your dbt Cloud Job.)
#
#  (4) Access the source data from dbt, e.g.,
#
#        select * from {{ source('jaffle_shop', 'customers') }}
# 
#  OTHER EXTERNAL TABLES
#
#  This package works with Snowpipes, GCS buckets, Redshift Spectrum tables, and more!
#
#  MORE INFO
#
#  - dbt package: https://github.com/dbt-labs/dbt-external-tables/
#  - external tables: https://docs.snowflake.com/en/user-guide/tables-external-intro.html
#

version: 2

sources:
  - name: jaffle_shop
    description: This is an example of using external sources
    database: analytics

    tables:
      - name: customers
        description: To be completed

        external:
          location: "@s3_dbt_tutorial_public"
          file_format: "(type=csv field_delimiter=',' skip_header=1)"
          auto_refresh: false
          # This is a static pattern, but it could be a star pattern
          pattern: 'jaffle_shop_customers.csv'

          partitions:
            - name: source_file_name
              data_type: varchar
              expression: metadata$filename

        columns:
          - name: ID
            data_type: NUMBER
            description: TBC
          - name: FIRST_NAME
            data_type: VARCHAR
            description: TBC
          - name: LAST_NAME
            data_type: VARCHAR
            description: TBC

      - name: orders
        description: To be completed

        external:
          location: "@s3_dbt_tutorial_public"
          file_format: "(type=csv field_delimiter=',' skip_header=1)"
          auto_refresh: false
          pattern: 'jaffle_shop_orders.csv'

          partitions:
            - name: source_file_name
              data_type: varchar
              expression: metadata$filename

        columns:
          - name: ID
            data_type: NUMBER
            description: TBC
          - name: USER_ID
            data_type: NUMBER
            description: TBC
          - name: ORDER_DATE
            data_type: DATE
            description: TBC
          - name: STATUS
            data_type: VARCHAR
            description: TBC

  - name: stripe
    description: This is an example of using external sources
    database: analytics
    schema: stripe_external_source

    tables:
      - name: payments
        description: To be completed

        external:
          location: "@s3_dbt_tutorial_public"
          file_format: "(type=csv field_delimiter=',' skip_header=1)"
          auto_refresh: false
          pattern: 'stripe_payments.csv'

          partitions:
            - name: source_file_name
              data_type: varchar
              expression: metadata$filename

        columns:
          - name: ID
            data_type: NUMBER
            description: TBC
          - name: ORDERID
            data_type: NUMBER
            description: TBC
          - name: PAYMENTMETHOD
            data_type: VARCHAR
            description: TBC
          - name: STATUS
            data_type: VARCHAR
            description: TBC
          - name: AMOUNT
            data_type: NUMBER
            description: TBC
          - name: CREATED
            data_type: DATE
            description: TBC
